{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 53569,
          "databundleVersionId": 5834979,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30497,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityar2309/ADITYAR-MOVIE-SENTIMENT-SCIKITLEARN/blob/main/ADITYA_Movie_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'sentiment-prediction-on-movie-reviews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F53569%2F5834979%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240311%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240311T065453Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D77cefae049a874010b56496b986039e3b4b2ba6e03f2288ef20460ea4687eea998cf74b08cae0ef10550c7488272bd4a9d9e4149b84850654d48590fd5e08ceda5b89df2e767443089dfa5637814c7711057c06c265609ae8e678eea97f2005fe817866bcaf90304054c72c580c739de8475d01f85512e4e833567c3b3f5421305e74e45f8e92a845c0cb49f018d3e0d5d6078a121246d8dd2c665ea741885c5bcd74b12981fe1ea36d6a4a72193cdc316db245543e7663089eed4344c116d2f3b73754e733b55a50483848a3f7d19b62817a9074d999d3f48da26ceab5d58e29eb8f234704df6a9ce64e0c8daabd191b2e0082c8e3e81e96fe28898087a7416'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lMOBv1Z5Sv7V"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import  StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:18.765495Z",
          "iopub.execute_input": "2023-08-23T12:55:18.765848Z",
          "iopub.status.idle": "2023-08-23T12:55:18.772777Z",
          "shell.execute_reply.started": "2023-08-23T12:55:18.765822Z",
          "shell.execute_reply": "2023-08-23T12:55:18.771582Z"
        },
        "trusted": true,
        "id": "UGXvc7U8Sv7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_data = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv')\n",
        "movie_data = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:18.775411Z",
          "iopub.execute_input": "2023-08-23T12:55:18.775852Z",
          "iopub.status.idle": "2023-08-23T12:55:19.756384Z",
          "shell.execute_reply.started": "2023-08-23T12:55:18.775815Z",
          "shell.execute_reply": "2023-08-23T12:55:19.755207Z"
        },
        "trusted": true,
        "id": "OQHwcPOrSv7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate movie entries\n",
        "movie_data = movie_data.drop_duplicates(subset=['movieid'], keep='first')\n",
        "# Merge train and movie datasets\n",
        "train_merged = train_data.merge(movie_data,on=\"movieid\",how='left')\n",
        "test_merged = test_data.merge(movie_data,on=\"movieid\",how='left')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:19.757747Z",
          "iopub.execute_input": "2023-08-23T12:55:19.758079Z",
          "iopub.status.idle": "2023-08-23T12:55:20.159917Z",
          "shell.execute_reply.started": "2023-08-23T12:55:19.758052Z",
          "shell.execute_reply": "2023-08-23T12:55:20.15845Z"
        },
        "trusted": true,
        "id": "2ZIbNfafSv7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics of the merged dataset\n",
        "print(train_merged.describe())\n",
        "\n",
        "# Percentage of null values in each column\n",
        "print((train_merged.isnull().sum() / len(train_data)) * 100)\n",
        "\n",
        "# Plot histogram of audience scores\n",
        "plt.hist(train_merged['audienceScore'])\n",
        "plt.xlabel('Audience Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Audience Scores')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:20.162726Z",
          "iopub.execute_input": "2023-08-23T12:55:20.163739Z",
          "iopub.status.idle": "2023-08-23T12:55:20.746103Z",
          "shell.execute_reply.started": "2023-08-23T12:55:20.163661Z",
          "shell.execute_reply": "2023-08-23T12:55:20.744388Z"
        },
        "trusted": true,
        "id": "4Bw2FsbySv7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p> <h3>Summary Statistics: </h3>\n",
        "        The audienceScore column has an average (mean) audience score of around 65, with a standard deviation of about 19.9.\n",
        "        The runtimeMinutes column has an average runtime of approximately 107 minutes, with a standard deviation of around 22.1.\n",
        "        The audienceScore ranges from 0 to 100, with quartiles at 51, 68, and 82.\n",
        "        The runtimeMinutes values vary from 4 to 561 minutes.\n",
        "    </p>\n",
        "<p>\n",
        "<h3> Null Values: </h3>\n",
        "        The percentage of missing values in each column is shown. For example, reviewText has around 3.96% missing values, and rating and ratingContents have about 39.15% missing values.\n",
        "    </p>"
      ],
      "metadata": {
        "id": "dL2GIrWqSv7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features and target\n",
        "y_train = train_merged['sentiment']\n",
        "X_train = train_merged.drop(columns = ['movieid', 'isFrequentReviewer',\n",
        "       'releaseDateTheaters', 'releaseDateStreaming', 'runtimeMinutes', 'boxOffice',\n",
        "       'soundType','sentiment', 'ratingContents'])# 'originalLanguage' ,'title','genre','reviewerName','director','distributor', 'rating'])\n",
        "X_test = test_merged.drop(columns = ['movieid', 'isTopCritic',\n",
        "       'releaseDateTheaters', 'releaseDateStreaming', 'runtimeMinutes', 'boxOffice',\n",
        "       'soundType', 'ratingContents'])#'originalLanguage','title','genre','reviewerName','director','distributor', 'rating',])\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:20.747685Z",
          "iopub.execute_input": "2023-08-23T12:55:20.748023Z",
          "iopub.status.idle": "2023-08-23T12:55:20.804097Z",
          "shell.execute_reply.started": "2023-08-23T12:55:20.747994Z",
          "shell.execute_reply": "2023-08-23T12:55:20.802843Z"
        },
        "trusted": true,
        "id": "ziN9fGEASv7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define numeric and categorical features\n",
        "numeric_features = ['audienceScore']\n",
        "categorical_features = ['reviewText', 'title', 'rating','genre', 'reviewerName', 'originalLanguage', 'distributor', 'director']\n",
        "\n",
        "# Impute missing values in categorical features\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "for feature in categorical_features:\n",
        "    X_train[feature] = imputer.fit_transform(X_train[[feature]])\n",
        "    X_test[feature] = imputer.transform(X_test[[feature]])\n",
        "\n",
        "# imputer = SimpleImputer(strategy='mean')\n",
        "# X_train['audienceScore'] = imputer.fit_transform(X_train[['audienceScore']])\n",
        "# X_test['audienceScore'] = imputer.transform(X_test[['audienceScore']])\n",
        "\n",
        "# # X_train[['reviewText']]=X_train[['reviewText']].fillna(\"\")\n",
        "# # X_train[['title']]=X_train[['title']].fillna(\"\")\n",
        "# # X_train[['genre']]=X_train[['genre']].fillna(\"\")\n",
        "# # X_train[['reviewerName']] = X_train[['reviewerName']].fillna(\"\")\n",
        "# # X_train['reviewerName']] = X_train[['originalLanguage']].fillna(\"\")\n",
        "# # X_train[['director']] =X_train[['director']].fillna(\"\")\n",
        "# # X_train[['distributor']] =X_train[['distributor']].fillna(\"\")\n",
        "# # X_test[['reviewText']]=X_test[['reviewText']].fillna(\"\")\n",
        "# # X_test[['title']]=X_test[['title']].fillna(\"\")\n",
        "# # X_test[['genre']]=X_test[['genre']].fillna(\"\")\n",
        "# # X_test[['reviewerName']] = X_test[['reviewerName']].fillna(\"\")\n",
        "# # X_test[['reviewerName']] = X_test[['originalLanguage']].fillna(\"\")\n",
        "# # X_test[['director']] =X_test[['director']].fillna(\"\")\n",
        "# # X_test[['distributor']] =X_test[['distributor']].fillna(\"\")\n",
        "# scaler = StandardScaler()\n",
        "# X_train['audienceScore'] = scaler.fit_transform(X_train[['audienceScore']])\n",
        "# X_test['audienceScore'] = scaler.transform(X_test[['audienceScore']])\n",
        "\n",
        "# tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['reviewText'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['reviewText'])\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['ratingContents'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['ratingContents'])\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['title'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['title'])\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['genre'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['genre'])\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['reviewerName'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['reviewerName'])\n",
        "\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['originalLanguage'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['originalLanguage'])\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['distributor'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['distributor'])\n",
        "\n",
        "\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['director'])\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test['director'])\n",
        "\n",
        "\n",
        "# Define transformers for numeric and categorical features\n",
        "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('tfidf',TfidfVectorizer())])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:20.806095Z",
          "iopub.execute_input": "2023-08-23T12:55:20.80657Z",
          "iopub.status.idle": "2023-08-23T12:55:21.612754Z",
          "shell.execute_reply.started": "2023-08-23T12:55:20.806534Z",
          "shell.execute_reply": "2023-08-23T12:55:21.611639Z"
        },
        "trusted": true,
        "id": "LoM7kcUMSv7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat0', categorical_transformer, 'reviewText'),\n",
        "        ('cat1', categorical_transformer, 'title'),\n",
        "        ('cat2', categorical_transformer, 'genre'),\n",
        "        ('cat3', categorical_transformer, 'reviewerName'),\n",
        "        ('cat4', categorical_transformer, 'originalLanguage'),\n",
        "        ('cat5', categorical_transformer, 'distributor'),\n",
        "        ('cat6', categorical_transformer, 'director')\n",
        "\n",
        "    ])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:21.614064Z",
          "iopub.execute_input": "2023-08-23T12:55:21.614404Z",
          "iopub.status.idle": "2023-08-23T12:55:21.619911Z",
          "shell.execute_reply.started": "2023-08-23T12:55:21.614379Z",
          "shell.execute_reply": "2023-08-23T12:55:21.619139Z"
        },
        "trusted": true,
        "id": "FaSqPLW9Sv7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models and hyperparameter distributions for RandomizedSearchCV\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_param_distributions = {\n",
        "    'C': [1.5, 2, 3],\n",
        "    'solver': ['saga'],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "svc_model = LinearSVC(max_iter=1000)\n",
        "svc_param_distributions = {\n",
        "    'C': [0.3, 0.4, 0.5, 1.2],\n",
        "    'tol': [0.001, 0.0001],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "sgd_logreg_model = SGDClassifier(loss='log_loss')\n",
        "param_distributions = {\n",
        "    'alpha': [1, 1.2],\n",
        "    'max_iter': [1000]\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:21.621277Z",
          "iopub.execute_input": "2023-08-23T12:55:21.621639Z",
          "iopub.status.idle": "2023-08-23T12:55:21.63626Z",
          "shell.execute_reply.started": "2023-08-23T12:55:21.621608Z",
          "shell.execute_reply": "2023-08-23T12:55:21.635002Z"
        },
        "trusted": true,
        "id": "H6vtbxzkSv7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# logreg_model = LogisticRegression()\n",
        "# logreg_param_distributions = {\n",
        "#     'C': [1.5,2,3],\n",
        "#     'solver':['saga'],\n",
        "#     'max_iter':[1000]\n",
        "# }\n",
        "\n",
        "# svc_model = LinearSVC(max_iter=1000)\n",
        "# svc_param_distributions = {\n",
        "#     'C': [0.3,0.4,0.5,1.2],\n",
        "#     'tol':[0.001,0.0001],\n",
        "#     'max_iter':[1000]\n",
        "# }\n",
        "\n",
        "# sgd_logreg_model = SGDClassifier(loss='log')\n",
        "# param_distributions = {\n",
        "#     'alpha': [1,1.2],\n",
        "#     'max_iter': [1000]\n",
        "# }\n",
        "\n",
        "# logreg_random_search = RandomizedSearchCV(logreg_model, logreg_param_distributions, n_iter=3, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "# logreg_random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# svc_random_search = RandomizedSearchCV(svc_model, svc_param_distributions, n_iter=8, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "# svc_random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# sgd_random_search = RandomizedSearchCV(sgd_logreg_model, param_distributions, n_iter=3, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "# sgd_random_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# print(logreg_random_search.best_params_)\n",
        "# print(svc_random_search.best_params_)\n",
        "# print(sgd_random_search.best_params_)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:21.637628Z",
          "iopub.execute_input": "2023-08-23T12:55:21.637892Z",
          "iopub.status.idle": "2023-08-23T12:55:21.655686Z",
          "shell.execute_reply.started": "2023-08-23T12:55:21.637867Z",
          "shell.execute_reply": "2023-08-23T12:55:21.654006Z"
        },
        "trusted": true,
        "id": "hHKI9FADSv7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build pipelines for each model\n",
        "logreg_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomizedSearchCV(logreg_model, logreg_param_distributions, n_iter=2, cv=3, scoring='accuracy', verbose=1, n_jobs=-1))\n",
        "])\n",
        "\n",
        "svc_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomizedSearchCV(svc_model, svc_param_distributions, n_iter=4, cv=3, scoring='accuracy', verbose=1, n_jobs=-1))\n",
        "])\n",
        "\n",
        "sgd_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomizedSearchCV(sgd_logreg_model, param_distributions, n_iter=2, cv=3, scoring='accuracy', verbose=1, n_jobs=-1))\n",
        "])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:21.658416Z",
          "iopub.execute_input": "2023-08-23T12:55:21.658676Z",
          "iopub.status.idle": "2023-08-23T12:55:21.677201Z",
          "shell.execute_reply.started": "2023-08-23T12:55:21.658653Z",
          "shell.execute_reply": "2023-08-23T12:55:21.676034Z"
        },
        "trusted": true,
        "id": "Oh6lVR0eSv7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit pipelines on training data\n",
        "logreg_pipeline.fit(X_train, y_train)\n",
        "svc_pipeline.fit(X_train, y_train)\n",
        "sgd_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Fit pipelines on training data\n",
        "print(logreg_pipeline.named_steps['classifier'].best_params_)\n",
        "print(svc_pipeline.named_steps['classifier'].best_params_)\n",
        "print(sgd_pipeline.named_steps['classifier'].best_params_)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-23T12:55:21.678478Z",
          "iopub.execute_input": "2023-08-23T12:55:21.678851Z"
        },
        "trusted": true,
        "id": "iRYisnDdSv7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Insights from Model Pipelines and Hyperparameter Tuning\n",
        "</h3>\n",
        "Three pipelines were built to train different models, namely Logistic Regression (LogReg), Support Vector Classifier (SVC), and Stochastic Gradient Descent (SGD). These pipelines consisted of two key stages:\n",
        "\n",
        "<h3>Preprocessing:</h3>\n",
        "        A preprocessing step was applied to the  data using the preprocessor component.\n",
        "        This preprocessor included tasks such as data scaling, encoding categorical features, and handling missing values to prepare the data for modeling.\n",
        "\n",
        "<h3>Model Training with Hyperparameter Tuning:</h3>\n",
        "        Each pipeline incorporated a classifier component along with hyperparameter tuning using RandomizedSearchCV.\n",
        "        RandomizedSearchCV randomly samples a specified number of hyperparameter combinations from the given parameter distributions and performs cross-validation to find the best combination that maximizes accuracy.\n",
        "        For the LogReg model, 3 iterations were performed with 3-fold cross-validation and the best parameters were selected based on accuracy.\n",
        "        For the SVC model, 8 iterations were performed with 3-fold cross-validation and the best parameters were chosen for accuracy.\n",
        "        The SGD model underwent 3 iterations with 3-fold cross-validation to identify the best parameters for accuracy.\n",
        "\n",
        "After fitting the pipelines to the training data, the best hyperparameters were extracted from each model's classifier component:\n",
        "\n",
        "<h3>Best hyperparameters for LogReg:</h3>\n",
        "        'solver': 'saga'\n",
        "        'max_iter': 1000\n",
        "        'C': 2\n",
        "\n",
        "   <h3>Best hyperparameters for SVC:</h3>\n",
        "        'tol': 0.001\n",
        "        'max_iter': 1000\n",
        "        'C': 0.3\n",
        "\n",
        "   <h3>Best hyperparameters for SGD:</h3>\n",
        "        'max_iter': 1000\n",
        "        'alpha': 1\n",
        "\n",
        "These insights provide valuable information about the selected hyperparameters for each model, allowing for better understanding and reproducibility of the model training process."
      ],
      "metadata": {
        "id": "7nhcY6kWSv7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment labels on training data\n",
        "y_pred_logreg = logreg_pipeline.predict(X_train)\n",
        "y_pred_svc = svc_pipeline.predict(X_train)\n",
        "y_pred_sgd = sgd_pipeline.predict(X_train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IY6mEAK3Sv7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print accuracies on training data\n",
        "accuracy_logreg = accuracy_score(y_train, y_pred_logreg)\n",
        "accuracy_svc = accuracy_score(y_train, y_pred_svc)\n",
        "accuracy_sgd = accuracy_score(y_train, y_pred_sgd)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z-M5PhpNSv7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_logreg)\n",
        "print(accuracy_svc)\n",
        "print(accuracy_sgd)"
      ],
      "metadata": {
        "trusted": true,
        "id": "XhdDKHdpSv7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# y_pred_logreg = logreg_random_search.predict(X_train_tfidf)\n",
        "# y_pred_svc = svc_random_search.predict(X_train_tfidf)\n",
        "# y_pred_sgd = sgd_random_search.predict(X_train_tfidf)\n",
        "\n",
        "\n",
        "# # Calculate accuracies\n",
        "# accuracy_logreg = accuracy_score(y_train, y_pred_logreg)\n",
        "# accuracy_svc = accuracy_score(y_train, y_pred_svc)\n",
        "# accuracy_sgd = accuracy_score(y_train, y_pred_sgd)\n",
        "\n",
        "\n",
        "# # Print accuracies\n",
        "# print(accuracy_logreg)\n",
        "# print(accuracy_svc)\n",
        "# print(accuracy_sgd)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "bz96nkW4Sv7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment labels on test data\n",
        "y_pred_logreg_test = logreg_pipeline.predict(X_test)\n",
        "y_pred_svc_test = svc_pipeline.predict(X_test)\n",
        "y_pred_sgd_test = sgd_pipeline.predict(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "20D5KN5tSv7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a submission DataFrame and save to CSV\n",
        "submission = pd.DataFrame({ 'sentiment': y_pred_svc_test})\n",
        "submission.to_csv('submission.csv', index_label='id')"
      ],
      "metadata": {
        "trusted": true,
        "id": "wL8WV7NKSv7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}